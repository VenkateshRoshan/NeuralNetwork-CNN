{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### To check whether weights are updated or not\n",
    "f = open('CNN.txt','w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,Y_train) , (X_test,Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train , X_test = X_train/255. , X_test/255."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "shape=(3,3)\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "Path = ['D:/Data/MultiDomain/Dataset/Animals/cats/','D:/Data/MultiDomain/Dataset/Animals/dogs/']#,'D:/Data/MultiDomain/Dataset/Animals/fox/']\n",
    "for i in Path :\n",
    "    co = 0\n",
    "    for j in os.listdir(i) :\n",
    "        img = cv2.imread(i+j)\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img,(shape[:2]))\n",
    "        co += 1\n",
    "        if co >= 50 and co <= 70 :\n",
    "            X_test.append(img)\n",
    "            Y_test.append(Path.index(i))\n",
    "        elif co < 50 :\n",
    "            X_train.append(img)\n",
    "            Y_train.append(Path.index(i))\n",
    "        else :\n",
    "            break\n",
    "X_train = np.array(X_train)/255.\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)/255.\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (60000, 28, 28), Y_train shape : (60000,)\n",
      "X_test shape : (10000, 28, 28), Y_test shape : (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape : {X_train.shape}, Y_train shape : {Y_train.shape}')\n",
    "print(f'X_test shape : {X_test.shape}, Y_test shape : {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU :\n",
    "    def __init__ (self) :\n",
    "        self.__type__ = 'activation'\n",
    "        self.__Name__ = 'ReLU'\n",
    "    \n",
    "    def feed(self,X) :\n",
    "        self.input = X\n",
    "        self.input_shape=X.shape\n",
    "        self.output = np.maximum(0,X)\n",
    "        self.output_shape = self.input_shape\n",
    "        return self.output\n",
    "    \n",
    "    def feed_back(self,Z,grad_output,lr) :\n",
    "        grad = Z > 0\n",
    "        #print(grad.shape,grad_output.shape)\n",
    "        return grad_output*grad\n",
    "    \n",
    "class Softmax :\n",
    "    def __init__ (self) :\n",
    "        self.__type__ = 'activation'\n",
    "        self.__Name__ = 'Softmax'\n",
    "    \n",
    "    def feed(self,X) :\n",
    "        self.input = X\n",
    "        e_x = np.exp(X-np.max(X))\n",
    "        self.output = e_x/e_x.sum()\n",
    "        return self.output\n",
    "    \n",
    "    def grad_feed(self,X) :\n",
    "        e_x = np.exp(X)\n",
    "        return (e_x/e_x.sum()) - (e_x**2/(e_x.sum()**2))\n",
    "    \n",
    "    def feed_back(self,Z,grad_output,lr) :\n",
    "        e_x = np.exp(Z)\n",
    "        out = e_x/ex.sum()\n",
    "        grad = out*(1-out)\n",
    "        #grad = e_x/e_x.sum()**2 - (e_x**2/(e_x.sum()**2))\n",
    "        return grad_output*grad\n",
    "    \n",
    "class Sigmoid :\n",
    "    def __init__ (self) :\n",
    "        self.__type__ = 'activation'\n",
    "        self.__Name__ = 'Sigmoid'\n",
    "    \n",
    "    def feed(self,X) :\n",
    "        self.input = X\n",
    "        self.output = 1/(1+np.exp(-X))\n",
    "        return self.output\n",
    "    \n",
    "    def feed_back(self,Z,grad_output,lr) :\n",
    "        out = 1/(1+np.exp(-Z))\n",
    "        grad = out*((1-out)**2)\n",
    "        return grad_output*grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Convolution` Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MaxPooling` Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Average Pooling` Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Flatten` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten :\n",
    "    \n",
    "    def __init__ (self,input_shape=None) :\n",
    "        self.__Name__ = 'Flatten'\n",
    "        self.__type__ = 'flat'\n",
    "        self.input_shape = input_shape\n",
    "        self.A_F = None\n",
    "        re = 1\n",
    "        for i in input_shape :\n",
    "            re *= i\n",
    "        self.output_shape = re\n",
    "        \n",
    "    def feed(self,X) :\n",
    "        self.input = X\n",
    "        self.output = X.ravel()\n",
    "        return self.output\n",
    "    \n",
    "    def Summary(self) :\n",
    "        l = len(self.__Name__)\n",
    "        print(f'{self.__Name__}',' '*(20-l),self.input_shape,' '*(20-len(str(self.input_shape))),self.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dense` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense :\n",
    "    \n",
    "    def __init__ (self,input_shape,N_F,A_F=None) :\n",
    "        self.__Name__ = 'Dense'\n",
    "        self.__type__ = 'dense'\n",
    "        self.input_shape = input_shape\n",
    "        self.N_F = N_F\n",
    "        self.A_F = A_F\n",
    "        self.output_shape = N_F\n",
    "        self.weights = np.random.randn(self.input_shape, self.output_shape) / np.sqrt(self.input_shape + self.output_shape)\n",
    "        self.bias = np.random.randn(1, self.output_shape) / np.sqrt(self.input_shape + self.output_shape)\n",
    "        #self.bias = np.random.randint(0,2,(1,self.output_shape)) / np.sqrt(self.input_shape + self.output_shape)\n",
    "        \n",
    "    def feed(self,X) :\n",
    "        if X.shape[0] != 1 :\n",
    "            output = []\n",
    "            output.append(X)\n",
    "            self.input = np.array(output)\n",
    "        else :\n",
    "            self.input = X\n",
    "        self.output = np.dot(X,self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def feed_back(self, Z , output_error, learning_rate=1e-03):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        #output_error = output_error.mean(axis=0)*Z.T.shape[0]\n",
    "        weights_error = np.dot(Z.T, output_error)\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "    \n",
    "#     def backward(self,grad_output,lr):\n",
    "#         grad_input = np.dot(grad_output, self.weights.T)\n",
    "#         grad_weights = np.dot(self.input.T, grad_output)\n",
    "#         grad_biases = grad_output.mean(axis=0)*input.shape[0]\n",
    "#         assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "#         self.weights = self.weights - self.learning_rate * grad_weights\n",
    "#         self.biases = self.biases - self.learning_rate * grad_biases\n",
    "#         return grad_input\n",
    "    \n",
    "    def Summary(self) :\n",
    "        l = len(self.__Name__)\n",
    "        print(f'{self.__Name__}',' '*(20-l),self.input_shape,' '*(20-len(str(self.input_shape))),self.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential :\n",
    "    \n",
    "    def __init__ (self) :\n",
    "        self.Layers = []\n",
    "        self.input_shape = None\n",
    "        self.Activations = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.error = []\n",
    "        self.val_error = []\n",
    "        \n",
    "    def add(self,Layer) :\n",
    "        self.Layers.append(Layer)\n",
    "        if Layer.__type__ != 'activation' :\n",
    "            if self.input_shape is None :\n",
    "                self.input_shape = Layer.input_shape\n",
    "            self.output_shape = Layer.output_shape\n",
    "        if Layer.A_F is not None :\n",
    "            if Layer.A_F.lower() == 'softmax' :\n",
    "                self.Activations.append(Softmax())\n",
    "            elif Layer.A_F.lower() == 'sigmoid' :\n",
    "                self.Activations.append(Sigmoid())\n",
    "            else :\n",
    "                self.Activations.append(ReLU())\n",
    "        else :\n",
    "            self.Activations.append(None)\n",
    "        \n",
    "    def compile(self,loss='cross_entropy',metrics=['acc']) :\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        \n",
    "    def one_hot_encode(self,labels) :\n",
    "        Labels = np.zeros((len(labels),len(set(labels))))\n",
    "        for i,label in enumerate(labels) :\n",
    "            Labels[i][label] = 1\n",
    "        return Labels\n",
    "        \n",
    "    def fit(self,train_data,valid_data=None,validation_split=.1,epochs=10,lr=0.01) :\n",
    "        self.epochs = epochs\n",
    "        if train_data is None :\n",
    "            raise ValueError('Training Data Required')\n",
    "        else :\n",
    "            self.input = train_data[0]\n",
    "            #self.Total = self.one_hot_encode(train_data[1])\n",
    "            self.Total = train_data[1]\n",
    "            self.target = self.Total\n",
    "            self.No_of_outs = len(set(self.target))\n",
    "        N = len(self.input)\n",
    "        \n",
    "        if valid_data is None :\n",
    "            if validation_split != 0 :\n",
    "                n = int(len(self.input)*(1-validation_split))\n",
    "                K = np.random.randint(0,N,(N))\n",
    "                for i in range(N-1) :\n",
    "                    train_data[0][K[i]] = train_data[0][K[i+1]]\n",
    "                    train_data[1][K[i]] = train_data[1][K[i+1]]\n",
    "                self.input , self.target = train_data[0][:n] , self.Total[:n]\n",
    "                self.val_input , self.val_target = train_data[0][n:] , self.Total[n:]\n",
    "        else :\n",
    "            self.val_input , self.val_target = valid_data[0] , valid_data[1]\n",
    "        \n",
    "        print('\\nModel Fitting\\n')\n",
    "        \n",
    "        for ep in range(epochs) :\n",
    "            error = 0\n",
    "            acc = 0\n",
    "            \n",
    "            print(f'\\nepoch : {ep+1}/{epochs}')\n",
    "            \n",
    "            for c,(X,Y) in enumerate(zip(self.input,self.target)) :\n",
    "                \n",
    "                L_INPUTS , L_OUTPUTS ,  A_INPUTS , A_OUTPUTS = [] , [] , [] , []\n",
    "                \n",
    "                output = X\n",
    "                loss = None\n",
    "                out_err = None\n",
    "                \n",
    "                \"\"\"\n",
    "                    Forward Feeding [ Z = W*X + B ]\n",
    "                \"\"\"\n",
    "                for layer , activation in zip(self.Layers,self.Activations) :\n",
    "                    L_INPUTS.append(output)\n",
    "                    output = layer.feed(output) # Feeding to Layer\n",
    "                    L_OUTPUTS.append(output)\n",
    "                    if activation is not None :\n",
    "                        A_INPUTS.append(output)\n",
    "                        output = activation.feed(output) # applying activation to output of the Layers\n",
    "                        A_OUTPUTS.append(output)\n",
    "                        \n",
    "                activation_output = self.Activations[-1].output # a-l\n",
    "                output = self.Layers[-1].output # z-l\n",
    "\n",
    "                \"\"\"\n",
    "                    Loss Calculation or Output Error\n",
    "                \"\"\"\n",
    "                \n",
    "                if self.loss == 'cross_entropy' :\n",
    "                    loss = self.crossentropy(activation_output,Y)\n",
    "                    grad_activation = self.Activations[-1].grad_feed(output)\n",
    "                    out_err = self.grad_crossentropy(output,Y)*grad_activation\n",
    "                \n",
    "                \"\"\"\n",
    "                    Backward Feeding\n",
    "                \"\"\"\n",
    "                \n",
    "                for i in range(1,len(self.Layers)-1) :\n",
    "                    if self.Layers[-i].__Name__ != 'Flatten' :\n",
    "                        if self.Activations[-i].__Name__ != 'Softmax' :\n",
    "                            out_err = self.Activations[-i].feed_back(A_INPUTS[-i],out_err,lr)\n",
    "                        out_err = self.Layers[-i].feed_back(L_INPUTS[-i],out_err,lr)\n",
    "                \n",
    "                error = np.mean(loss)\n",
    "                print('\\rerror=%f' % (error),end=\"\")\n",
    "                \n",
    "            \"\"\"\n",
    "                Accuracy measuring at every epoch\n",
    "            \"\"\"\n",
    "            \n",
    "            accuracy = sum([y == np.argmax(model.predict(x)) for x, y in zip(self.input, self.target)]) / len(self.input)\n",
    "            self.acc.append(accuracy)\n",
    "            \n",
    "            if 'acc' in self.metrics :\n",
    "                val_accuracy = sum([y == np.argmax(model.predict(x)) for x, y in zip(self.val_input, self.val_target)]) / len(self.val_input)\n",
    "                self.val_acc.append(val_accuracy)\n",
    "                print(' acc=%f , val_acc=%f' % (accuracy , val_accuracy))\n",
    "            else :\n",
    "                print('\\racc=%f' % (accuracy))\n",
    "            \n",
    "        return None\n",
    "            \n",
    "    def mse(self,y_true, y_pred):\n",
    "        return np.mean(np.power(y_true - y_pred, 2))\n",
    "    \n",
    "    def mse_prime(self,y_true, y_pred):\n",
    "        return 2 * (y_pred - y_true) / y_pred.size\n",
    "    \n",
    "    def transfer_derivative(self,output):\n",
    "        return output * (1.0 - output)\n",
    "    \n",
    "#     def crossentropy(self,pred,Truth) :\n",
    "#         GroundTruth = np.zeros(self.No_of_outs)\n",
    "#         GroundTruth[Truth] = 1\n",
    "#         return np.mean(GroundTruth*np.log(pred)+(1-GroundTruth)*np.log(1-pred))\n",
    "    \n",
    "#     def grad_crossentropy(self,pred,Truth) :\n",
    "#         GroundTruth = np.zeros(self.No_of_outs)\n",
    "#         GroundTruth[Truth] = 1\n",
    "#         return ((GroundTruth/pred)-((1-GroundTruth)/(1-pred)))\n",
    "    \n",
    "    def crossentropy(self,logits,reference_answers):\n",
    "        #print(logits[0][reference_answers]+np.log(np.sum(np.exp(logits),axis=-1)))\n",
    "        return - logits[0][reference_answers] + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    def grad_crossentropy(self,logits,reference_answers):\n",
    "        ones_for_answers = np.zeros_like(logits)\n",
    "        ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "        softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "        return (- ones_for_answers + softmax) / logits.shape[0]\n",
    "    \n",
    "    def showImg(self,X) :\n",
    "        plt.imshow(X)\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self,X):\n",
    "        output = X\n",
    "        for layer , activation in zip(self.Layers,self.Activations) :\n",
    "            output = layer.feed(output)\n",
    "            if activation is not None :\n",
    "                output = activation.feed(output)\n",
    "        return output\n",
    "    \n",
    "    def pred_class(self,X) :\n",
    "        classes = []\n",
    "        if X.shape == model.input_shape :\n",
    "            output = self.predict(X)\n",
    "            return np.argmax(output)\n",
    "        else :\n",
    "            for output in X :\n",
    "                output = self.predict(output)\n",
    "                classes.append(np.argmax(output))\n",
    "            return np.array(classes)\n",
    "    \n",
    "    def Summary(self) :\n",
    "        print('='*60)\n",
    "        print('Model Summary')\n",
    "        print('_'*60)\n",
    "        print('Layers',' '*(20-len('Layers')),'Input Shape',' '*(20-len('Input Shape')),'Output Shape',' '*(20-len('Output Shape')))\n",
    "        print('='*60)\n",
    "        for Layer in self.Layers :\n",
    "            if Layer.__type__ != 'activation' :\n",
    "                Layer.Summary()\n",
    "                print('_'*60)\n",
    "        print('='*60)\n",
    "                \n",
    "    def save(self,path) :\n",
    "        f = open(path,'w')\n",
    "        for i in self.Layers :\n",
    "            if i.__Name__ != 'Flatten' :\n",
    "                f.write(i.__Name__,i.N_F,i.A_F,i.weights)\n",
    "                \n",
    "    def load_model(self,path) :\n",
    "        f = open(path,'r')\n",
    "        for i in self.Layers :\n",
    "            if i.__Name__ != 'Flatten' :\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Model Summary\n",
      "____________________________________________________________\n",
      "Layers                Input Shape           Output Shape         \n",
      "============================================================\n",
      "Flatten               (28, 28)              784\n",
      "____________________________________________________________\n",
      "Dense                 784                   100\n",
      "____________________________________________________________\n",
      "Dense                 100                   32\n",
      "____________________________________________________________\n",
      "Dense                 32                    10\n",
      "____________________________________________________________\n",
      "============================================================\n",
      "\n",
      "Model Fitting\n",
      "\n",
      "\n",
      "epoch : 1/10\n",
      "error=2.338188 acc=0.743241 , val_acc=0.762167\n",
      "\n",
      "epoch : 2/10\n",
      "error=2.194753"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c32236152b28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#,valid_data=(X_test,Y_test))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-796521341c8c>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_data, valid_data, validation_split, epochs, lr)\u001b[0m\n\u001b[0;32m     98\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                     \u001b[0mgrad_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mActivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                     \u001b[0mout_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad_activation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \"\"\"\n",
      "\u001b[1;32m<ipython-input-7-796521341c8c>\u001b[0m in \u001b[0;36mgrad_crossentropy\u001b[1;34m(self, logits, reference_answers)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgrad_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreference_answers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mones_for_answers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mones_for_answers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreference_answers\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m         \u001b[0msoftmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mones_for_answers\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=X_train[0].shape))\n",
    "model.add(Dense(input_shape=model.output_shape,N_F=100,A_F='ReLU'))\n",
    "model.add(Dense(input_shape=model.output_shape,N_F=32,A_F='ReLU'))\n",
    "model.add(Dense(input_shape=model.output_shape,N_F=len(set(Y_train)),A_F='Softmax'))\n",
    "model.compile(loss='cross_entropy',metrics=['acc'])\n",
    "\n",
    "model.Summary() \n",
    "model.fit(train_data=(X_train,Y_train),epochs=10,lr=1e-02,validation_split=.1)#,valid_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(model.epochs),model.acc,c='r',label='acc')\n",
    "plt.plot(range(model.epochs),model.val_acc,c='b',label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in range(len(X_test)) :\n",
    "    print(model.pred_class(X_test[i]),Y_test[i])\n",
    "    if model.pred_class(X_test[i]) == Y_test[i] :\n",
    "        c += 1\n",
    "print(c*100/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.pred_class(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pred_class(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in range(len(X_train)) :\n",
    "    #print(model.pred_class(X_train[i]),Y_train[i])\n",
    "    if model.pred_class(X_train[i]) == Y_train[i] :\n",
    "        c += 1\n",
    "print(c*100/len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = X_train[1]\n",
    "print(model.predict(img))\n",
    "print(model.pred_class(img))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = X_train[6]\n",
    "print(model.predict(img))\n",
    "print(model.pred_class(img))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

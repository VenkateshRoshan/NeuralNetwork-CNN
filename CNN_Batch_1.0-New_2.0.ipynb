{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Signals Detection Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "from tqdm.notebook import tnrange\n",
    "from IPython.display import clear_output\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loc = 'train_test_1.1.obj' #LoadedData_1.1.obj\n",
    "f = open(loc , 'rb')\n",
    "X_train , Y_train , X_test , Y_test , Labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'Model_Conv_2.0.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(obj,fileName) :\n",
    "    print('CallBack : ',fileName,'Updated')\n",
    "    f = open(fileName, 'wb') \n",
    "    pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(f'X_train shape : {X_train.shape}, Y_train shape : {Y_train.shape} , X_test shape : {X_test.shape}, Y_test shape : {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set(Y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU :\n",
    "    def __init__ (self) :\n",
    "        self.__type__ = 'activation'\n",
    "        self.__Name__ = 'ReLU'\n",
    "    \n",
    "    def feed(self,X) :\n",
    "        outputs = []\n",
    "        for x in X :\n",
    "            output = np.maximum(0,x)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    def feed_back(self,Z,grad_output,lr) :\n",
    "        grad = Z\n",
    "        grad[Z < 0] = 0\n",
    "        return grad_output*grad\n",
    "    \n",
    "    def predict(self,Y) :\n",
    "        output = np.maximum(0,Y)\n",
    "        return output\n",
    "    \n",
    "class TanH :\n",
    "    def __init__(self) :\n",
    "        self.__type__ = 'activation'\n",
    "        self.__Name__ = 'TanH'\n",
    "    def feed(self,X) :\n",
    "        outputs = []\n",
    "        for x in X :\n",
    "            output = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "        \n",
    "    def feed_back(self,Z,grad_output,lr) :\n",
    "        return 1-np.power(self.output,2)\n",
    "    \n",
    "    def predict(self,Y) :\n",
    "        output = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        return output\n",
    "        \n",
    "class Softmax :\n",
    "    def __init__ (self) :\n",
    "        self.__type__ = 'activation'\n",
    "        self.__Name__ = 'Softmax'\n",
    "    \n",
    "    def feed(self,X) :\n",
    "        outputs = []\n",
    "        for x in X :\n",
    "            e_x = np.exp(x - np.max(x))\n",
    "            output = e_x/np.sum(e_x)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    def grad_feed(self,X) :\n",
    "        e_x = np.exp(X - np.max(X))\n",
    "        out = (e_x/e_x.sum()) - ((e_x*e_x)/np.power(e_x.sum(),2))\n",
    "        return out\n",
    "    \n",
    "    def feed_back(self,Z,grad_output,lr) :\n",
    "        e_x = np.exp(Z - np.max(Z))\n",
    "        out = e_x/np.sum(e_x)\n",
    "        softmax = np.reshape(out, (1, -1))\n",
    "        grad_output = np.reshape(grad_output, (1, -1))\n",
    "        grad = (softmax * np.identity(softmax.size) - softmax.transpose() @ softmax)\n",
    "        return grad_output*grad\n",
    "    \n",
    "    def predict(self,Y) :\n",
    "        e_x = np.exp(Y-np.max(Y))\n",
    "        output = e_x/np.sum(e_x)\n",
    "        return output\n",
    "    \n",
    "class Sigmoid :\n",
    "    def __init__ (self) :\n",
    "        self.__type__ = 'activation'\n",
    "        self.__Name__ = 'Sigmoid'\n",
    "    \n",
    "    def feed(self,X) :\n",
    "        outputs = []\n",
    "        for x in X :\n",
    "            output = 1/(1+np.exp(-x))\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    def grad_feed(self,X) :\n",
    "        out = 1/(1+np.exp(-X))\n",
    "        return out*(1-out)\n",
    "    \n",
    "    def feed_back(self,Z,grad_output,lr) :\n",
    "        out = 1/(1+np.exp(-Z))\n",
    "        grad = out*((1-out)**2)\n",
    "        return grad_output*grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Conv2D :\n",
    "    \n",
    "    def __init__ (self,N_F,K_S,A_F,input_shape,STRIDES=1,pad=0) :\n",
    "        self.__Name__ = 'Conv2D'\n",
    "        self.__type__ = 'conv'\n",
    "        self.N_F = N_F\n",
    "        self.K_S = K_S\n",
    "        self.STRIDES = STRIDES\n",
    "        self.A_F = A_F\n",
    "        self.pad = pad\n",
    "        if (len(input_shape) == 2) :\n",
    "            self.input_shape = input_shape+(1,)\n",
    "        else :\n",
    "            self.input_shape = input_shape\n",
    "        self.input_shape = (None,) + self.input_shape\n",
    "        self.weights = np.random.normal(size=(self.N_F,self.input_shape[-1],self.K_S,self.K_S))\n",
    "        self.W = int((self.input_shape[1]-self.K_S+2*self.pad)/self.STRIDES) + 1\n",
    "        self.H = int((self.input_shape[2]-self.K_S+2*self.pad)/self.STRIDES) + 1\n",
    "        self.D = self.N_F\n",
    "        self.bias = np.random.randint(-1,2,(self.N_F))\n",
    "        self.output_shape = (None,) + (self.W,self.H,self.D)\n",
    "        self.Batch_W = []\n",
    "        self.Batch_B = []\n",
    "\n",
    "    def feed(self,X) :\n",
    "        outputs = []\n",
    "        for x in X :\n",
    "            output = np.zeros((self.W,self.H,self.D))\n",
    "            for i in range(0,self.W,self.STRIDES) :\n",
    "                for j in range(0,self.H,self.STRIDES) :\n",
    "                    cur_reg = x[i:i+self.K_S,j:j+self.K_S].T*self.weights\n",
    "                    output[i,j] = np.sum(cur_reg,axis=(1,2,3)) + self.bias\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    def feed_back(self, Z , output_error, learning_rate=[1e-03],decay=1e-03,opt='sgd') :\n",
    "        i_c,i_dim,_ = Z.shape\n",
    "        d_op = np.zeros(Z.shape)\n",
    "        d_conv = np.zeros(self.weights.shape)\n",
    "        d_bias = np.zeros(self.bias.shape)\n",
    "        out_y = 0\n",
    "        for f_y in range(0,i_dim-self.K_S,self.STRIDES) :\n",
    "            out_x = 0\n",
    "            for f_x in range(0,i_dim-self.K_S,self.STRIDES) :\n",
    "                roi = Z[:,f_y:f_y+self.K_S,f_x:f_x+self.K_S]\n",
    "                d_conv[fn] += output_error[fn,out_y,out_x] * roi\n",
    "                d_op[:,f_y:f_y+self.K_S,f_x:f_x+self.K_S] += output_error[fn,out_y,out_x] * self.weights[fn]\n",
    "                out_x += 1\n",
    "            out_y += 1\n",
    "            d_bias = np.sum(output_error,axis=1)\n",
    "        return d_conv , d_bias , d_op\n",
    "\n",
    "    def predict(self,Y) :\n",
    "        output = np.zeros((self.W,self.H,self.D))\n",
    "        for i in range(0,self.W,self.STRIDES) :\n",
    "            for j in range(0,self.H,self.STRIDES) :\n",
    "                cur_reg = Y[i:i+self.K_S,j:j+self.K_S].T*self.weights\n",
    "                output[i,j] = np.sum(cur_reg,axis=(1,2,3)) + self.bias\n",
    "#         print(output.shape)\n",
    "        return output\n",
    "    \n",
    "    def plotImg(self,X=None) :\n",
    "        if X is None :\n",
    "            X = self.output\n",
    "        Filter_SIZE = int(X.shape[-1]**(1/2))\n",
    "        _, axs = plt.subplots(Filter_SIZE,Filter_SIZE, figsize=(8,8))\n",
    "        axs = axs.flatten()\n",
    "        for i , ax in enumerate(axs) :\n",
    "            img = X[:,:,i]\n",
    "            ax.axis('off')\n",
    "            ax.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "    def Summary(self) :\n",
    "        l = len(self.__Name__)\n",
    "        print(f'{self.__Name__}',' '*(20-l),self.input_shape,' '*(20-len(str(self.input_shape))),self.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D :\n",
    "    \n",
    "    def __init__ (self,K_S,input_shape,STRIDES=2,pad=0) :\n",
    "        self.__Name__ = 'MaxPool2D'\n",
    "        self.__type__ = 'pool'\n",
    "        self.K_S = K_S\n",
    "        self.STRIDES = STRIDES\n",
    "        self.A_F = None\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (None,) + (int((self.input_shape[1]-self.K_S+1)/self.STRIDES),int((self.input_shape[2]-self.K_S+1)/self.STRIDES),self.input_shape[-1])\n",
    "        self.Batch_W = []\n",
    "        self.Batch_B = []\n",
    "        self.weights = np.zeros((self.input_shape[1:]))\n",
    "        self.bias = np.zeros((self.output_shape[1:]))\n",
    "        \n",
    "    def feed(self,X) :\n",
    "        outputs = []\n",
    "        for x in X :\n",
    "            W,H,D = self.output_shape[1:]\n",
    "            output = np.zeros((W,H,D))\n",
    "            w , h = 0 , 0\n",
    "            for i in range(0,self.input_shape[1]-self.K_S,self.STRIDES) :\n",
    "                for j in range(0,self.input_shape[2]-self.K_S,self.STRIDES) :\n",
    "                    output[w,h] = np.max(x[j:j+self.K_S,i:i+self.K_S].T,axis=(1,2))\n",
    "                    h += 1\n",
    "                h = 0\n",
    "                w += 1\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    def feed_back(self, Z , output_error, learning_rate=[1e-03],decay=1e-03,opt='sgd') :\n",
    "        i_c,i_dim,_ = Z.shape\n",
    "        d_op = np.zeros(Z.shape)\n",
    "        for cn in range(i_c) :\n",
    "            out_y = 0\n",
    "            for y in range(0,i_dim-self.K_S,self.STRIDES) :\n",
    "                out_x = 0\n",
    "                for x in range(0,i_dim-self.K_S,self.STRIDES) :\n",
    "                    try :\n",
    "                        roi = Z[cn,y:y+self.K_S,x:x+self.K_S]\n",
    "    #                     print('->',roi)\n",
    "                        update_y ,update_x = np.unravel_index(np.argmax(roi),roi.shape)\n",
    "                        d_op[cn,y+update_y,x+update_x] = output_error[cn,out_y,out_x]\n",
    "                    except :\n",
    "                        pass\n",
    "                    out_x += 1\n",
    "                out_y += 1\n",
    "        return self.weights,self.bias,d_op\n",
    "    \n",
    "    def predict(self,Y) :\n",
    "        W,H,D = self.output_shape[1:]\n",
    "        output = np.zeros((W,H,D))\n",
    "        w , h = 0 , 0\n",
    "        for i in range(0,self.input_shape[1]-self.K_S,self.STRIDES) :\n",
    "            for j in range(0,self.input_shape[2]-self.K_S,self.STRIDES) :\n",
    "                output[w,h] = np.max(Y[j:j+self.K_S,i:i+self.K_S].T,axis=(1,2))\n",
    "                h += 1\n",
    "            h = 0\n",
    "            w += 1\n",
    "        return output\n",
    "    \n",
    "    def plotImg(self,X=None) :\n",
    "        if X is None :\n",
    "            X = self.output\n",
    "        Filter_SIZE = int(X.shape[-1]**(1/2))\n",
    "        _, axs = plt.subplots(Filter_SIZE,Filter_SIZE, figsize=(8,8))\n",
    "        axs = axs.flatten()\n",
    "        for i , ax in enumerate(axs) :\n",
    "            img = X[:,:,i]\n",
    "            ax.axis('off')\n",
    "            ax.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "    def Summary(self) :\n",
    "        l = len(self.__Name__)\n",
    "        print(f'{self.__Name__}',' '*(20-l),self.input_shape,' '*(20-len(str(self.input_shape))),self.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten :\n",
    "    \n",
    "    \"\"\"\n",
    "        Flatten class is used to convert data into single dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self,input_shape=None,output_shape=None,__Name__='Flatten') : ### Constructor called when we create object \n",
    "        self.__Name__ = __Name__ ### Defining __Name__ variable with Flatten\n",
    "        self.__type__ = 'flat' ### Defining __type__ variable\n",
    "        self.input_shape = input_shape\n",
    "        self.A_F = None\n",
    "        re = 1\n",
    "        if output_shape is None :\n",
    "            for i in input_shape[1:] :\n",
    "                re *= i\n",
    "            self.output_shape = (None,re)\n",
    "        else :\n",
    "            self.output_shape = output_shape\n",
    "        self.Batch_W = []\n",
    "        self.Batch_B = []\n",
    "        self.weights = np.zeros((self.input_shape[1:]))\n",
    "        self.bias = np.zeros((self.output_shape[1:]))\n",
    "        \n",
    "    def feed(self,X) : ### feed function is used to transforms data into single dimension\n",
    "        outputs = []\n",
    "#         print(self.__Name__)\n",
    "        for x in X :\n",
    "            output = x.ravel() ### ravel is used to convert data into single dimensoin or flattens data\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    def feed_back(self, Z , output_error, learning_rate=[1e-03],decay=1e-03,opt='sgd') :\n",
    "        return (self.weights,self.bias,output_error.reshape(self.input_shape[1:]))\n",
    "    \n",
    "    def predict(self,Y) :\n",
    "        return Y.ravel()\n",
    "    \n",
    "    def Summary(self) :\n",
    "        l = len(self.__Name__)\n",
    "        print(f'{self.__Name__}',' '*(20-l),self.input_shape,' '*(20-len(str(self.input_shape))),self.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Dropout :\n",
    "    def __init__(self,input_shape=None,drop=.5,__Name__ = 'Dropout',A_F=None) :\n",
    "        self.__Name__ = __Name__\n",
    "        self.__type__ = 'drop'\n",
    "        self.N_F = drop\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = self.input_shape\n",
    "        self.weights = np.random.binomial(1, drop, self.input_shape[1:]) / drop\n",
    "        self.bias = np.zeros(self.input_shape[1:])\n",
    "        self.A_F = A_F\n",
    "        self.Batch_W = []\n",
    "        self.Batch_B = []\n",
    "        \n",
    "    def feed(self,X) :\n",
    "        outputs = []\n",
    "        for x in X :\n",
    "            output = x*self.weights.T\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    def feed_back(self, Z , output_error, learning_rate=[1e-03],decay=1e-03,opt='sgd') :\n",
    "        return (self.weights,self.bias , Z*output_error/self.N_F)\n",
    "    \n",
    "    def predict(self,Y) :\n",
    "        return Y*self.weights\n",
    "    \n",
    "    def Summary(self) :\n",
    "        l = len(self.__Name__)\n",
    "        print(f'{self.__Name__}',' '*(20-l),self.input_shape,' '*(20-len(str(self.input_shape))),self.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense :\n",
    "    \n",
    "    def __init__ (self,input_shape,N_F,A_F=None,wt=None,bias=None,output_shape=None,__Name__='Dense') :\n",
    "        \n",
    "        \"\"\"\n",
    "            Wt and Bias range [-2.4/No of nodes , 2.4/No of nodes] Proposed by range\n",
    "        \"\"\"\n",
    "        self.__Name__ = __Name__\n",
    "        self.__type__ = 'dense'\n",
    "        self.input_shape = input_shape\n",
    "        self.N_F = N_F\n",
    "        self.A_F = A_F\n",
    "        if output_shape is None :\n",
    "            self.output_shape = (None,N_F)\n",
    "        else :\n",
    "            self.output_shape = (None,) + output_shape\n",
    "        if wt is None :\n",
    "            self.weights = np.random.uniform(-2.4/self.N_F,2.4/self.N_F,(self.input_shape[1], self.output_shape[1]))\n",
    "        else :\n",
    "            self.weights = wt\n",
    "        if bias is None :\n",
    "            self.bias = np.random.uniform(-2.4/ self.N_F,2.4/ self.N_F,(1, self.output_shape[1]))\n",
    "        else :\n",
    "            self.bias = bias\n",
    "        self.Batch_W = []\n",
    "        self.Batch_B = []\n",
    "        self.v_ = np.zeros((self.input_shape[1], self.output_shape[1]))\n",
    "        self.s_ = np.zeros((self.input_shape[1], self.output_shape[1]))\n",
    "        self.vv_ = np.zeros((self.output_shape[1]))\n",
    "        self.ss_ = np.zeros((self.output_shape[1]))\n",
    "        \n",
    "    def feed(self,X) :\n",
    "        outputs = []\n",
    "        for x in X :\n",
    "            output = np.matmul(x.ravel(),self.weights) + self.bias\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "\n",
    "    def feed_back(self, Z , output_error, learning_rate=[1e-03],decay=1e-03,opt='sgd',b1=.9,b2=.999):\n",
    "        input_error = np.dot(output_error,self.weights.T)\n",
    "        weights , bias = None , None\n",
    "        weights_error = np.dot(Z.T, output_error)\n",
    "        lr = learning_rate[np.random.randint(len(learning_rate))]\n",
    "        if opt.lower() == 'gd' :\n",
    "            weights = self.weights - lr * weights_error\n",
    "            bias = self.bias - lr * output_error\n",
    "        elif opt.lower() == 'sgd' :\n",
    "            weights = self.weights - lr * weights_error - lr * (decay/self.N_F) * input_error\n",
    "            bias = self.bias - lr * (decay/self.N_F) * output_error\n",
    "        elif opt.lower() == 'rmsprop' :\n",
    "            self.v_ = b1 * self.v_ + .1 * np.power(weights_error,2)\n",
    "            weights = self.weights - lr * weights_error/(np.sqrt(np.maximum(self.v_,0))+decay)\n",
    "            bias = self.bias - lr * output_error\n",
    "        elif opt.lower() == 'adam' :\n",
    "            self.v_ = b1 * self.v_ + (1-b1) * weights_error\n",
    "            self.s_ = b2 * self.s_ + (1-b2) * (weights_error * weights_error)\n",
    "            self.V_ = self.v_ / (1-b1)\n",
    "            self.S_ = self.s_ / (1-b2)\n",
    "            weights = self.weights - lr * (self.V_ / (np.sqrt(np.maximum(self.S_,0))+ decay))\n",
    "            self.vv_ = b1 * self.vv_ + (1-b1) * output_error\n",
    "            self.ss_ = b2 * self.ss_ + (1-b2) * (output_error * output_error)\n",
    "            self.VV_ = self.vv_ / (1-b1)\n",
    "            self.SS_ = self.ss_ / (1-b2)\n",
    "            bias = self.bias - lr * (self.VV_ / (np.sqrt(np.maximum(self.SS_,0))+ decay))\n",
    "            #bias = self.bias - lr * output_error\n",
    "        return (weights , bias , input_error)\n",
    "    \n",
    "    def predict(self,Y) :\n",
    "        return np.matmul(Y,self.weights) + self.bias\n",
    "    \n",
    "    def Summary(self) :\n",
    "        l = len(self.__Name__)\n",
    "        print(f'{self.__Name__}',' '*(20-l),self.input_shape,' '*(20-len(str(self.input_shape))),self.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotAcc(model) : ### Accuracy Plot\n",
    "    plt.plot(range(model.ep),model.acc,c='b',label='acc')\n",
    "    plt.plot(range(model.ep),model.val_acc,c='r',label='val_acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def PlotError(model) : ### Error Plot\n",
    "    plt.plot(range(model.ep),model.error,c='b',label='loss')\n",
    "    plt.plot(range(model.ep),model.val_error,c='r',label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential :\n",
    "    \n",
    "    \"\"\"\n",
    "        Sequential is a class which is used to stack layers of model and to fit , predict , predicting classes of our given i/p\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self) :\n",
    "        self.Layers = []\n",
    "        self.input_shape = None\n",
    "        self.Activations = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.error = []\n",
    "        self.val_error = []\n",
    "        self.id = 1\n",
    "        self.ep = 1\n",
    "        \n",
    "    def add(self,Layer) :\n",
    "        boo = False\n",
    "        for layer in self.Layers :\n",
    "            if Layer.__type__ == layer.__type__ :\n",
    "                boo = True\n",
    "                if '_' not in Layer.__Name__ :\n",
    "                    Layer.__Name__ += '_'+str(self.id)\n",
    "                name,k = Layer.__Name__.split('_')\n",
    "                Layer.__Name__ = name+'_'+str(int(k)+1)\n",
    "        if not boo :\n",
    "            if '_' not in Layer.__Name__ :\n",
    "                Layer.__Name__ += '_'+str(self.id)\n",
    "        \n",
    "        self.Layers.append(Layer)\n",
    "        if Layer.__type__ != 'activation' :\n",
    "            if self.input_shape is None :\n",
    "                self.input_shape = Layer.input_shape\n",
    "            self.output_shape = Layer.output_shape\n",
    "        if Layer.A_F is not None :\n",
    "            if Layer.A_F.lower() == 'softmax' :\n",
    "                self.Activations.append(Softmax())\n",
    "            elif Layer.A_F.lower() == 'sigmoid' :\n",
    "                self.Activations.append(Sigmoid())\n",
    "            elif Layer.A_F.lower() == 'tanh' :\n",
    "                self.Activations.append(TanH())\n",
    "            else :\n",
    "                self.Activations.append(ReLU())\n",
    "        else :\n",
    "            self.Activations.append(None)\n",
    "    def ShuffleData(self,X,Y) :\n",
    "        order = np.random.randint(0,len(Y),(len(Y)))\n",
    "        #print(order)\n",
    "        for i in range(len(Y)-1) :\n",
    "            X[order[i]] , X[order[i+1]] = X[order[i+1]] , X[order[i]]\n",
    "            Y[order[i]] , Y[order[i+1]] = Y[order[i+1]] , Y[order[i]]\n",
    "        return (X , Y)\n",
    "    \n",
    "    def SplitData(self,X,Y,split) :\n",
    "        input_data , output_data , val_input_data , val_output_data = None , None , None , None\n",
    "        N = int(len(Y) * (1-split))\n",
    "        while True :\n",
    "            X , Y = self.ShuffleData(X,Y)\n",
    "            if set(Y[:N]) != set(Y[N:]) :\n",
    "                X ,Y = self.ShuffleData(X,Y)\n",
    "            else :\n",
    "                input_data , output_data = X[:N] , Y[:N]\n",
    "                val_input_data , val_output_data = X[N:] , Y[N:]\n",
    "                break\n",
    "        return (input_data , output_data , val_input_data , val_output_data)\n",
    "        \n",
    "    def compile(self,optimizer='sgd',loss='cross_entropy',metrics=['acc']) :\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        \n",
    "    def one_hot_encode(self,l) :\n",
    "        Labels = np.zeros(self.Layers[-1].output_shape[1:])\n",
    "        Labels[l] = 1\n",
    "#         for i,label in enumerate(labels) :\n",
    "#             Labels[i][label] = 1\n",
    "        return Labels\n",
    "\n",
    "    def trainModel_(self , b , X , Y , lr,decay) :\n",
    "        error = None\n",
    "        for x,y in zip(X,Y) :\n",
    "            output = x\n",
    "            loss = None\n",
    "            \n",
    "            for layer , activation in zip(self.Layers , self.Activations) :\n",
    "                output = layer.feed(output)\n",
    "                if activation is not None :\n",
    "                    output = activation.feed(output)\n",
    "                    \n",
    "            \n",
    "        return None\n",
    "\n",
    "    def trainModel(self , b , X , Y , lr,decay) : ### X and Y -> batch size of x and batch size of y and training model batch wise\n",
    "        error = None\n",
    "        nam = [i.__Name__ for i in self.Layers]\n",
    "        for x,y in zip(X,Y) :\n",
    "            # Training model\n",
    "            output = x\n",
    "            loss = None\n",
    "            \n",
    "            \"\"\"\n",
    "                Forward Feeding\n",
    "            \"\"\"\n",
    "            \n",
    "            L_INPUTS , L_OUTPUTS , A_INPUTS , A_OUTPUTS = [] , [] , [] , []\n",
    "            \n",
    "#             print('*Forward')\n",
    "            \n",
    "            for layer , activation in zip(self.Layers , self.Activations) :\n",
    "                L_INPUTS.append(output)\n",
    "#                 print(layer.__Name__)\n",
    "                output = layer.feed(output)\n",
    "#                 print(layer.__Name__,output.shape)\n",
    "                L_OUTPUTS.append(output)\n",
    "                A_INPUTS.append(output)\n",
    "                if activation is not None :\n",
    "                    output = activation.feed(output)\n",
    "#                     print(activation.__Name__,output.shape)\n",
    "                A_OUTPUTS.append(output)\n",
    "            \n",
    "            \"\"\"\n",
    "                Loss Calculation\n",
    "            \"\"\"\n",
    "#             print('*Loss')\n",
    "            \n",
    "            if self.loss == 'cross_entropy' :\n",
    "                loss = self.cat_crossentropy(A_OUTPUTS[-1],y)\n",
    "                if math.isnan(loss) :\n",
    "                    return None\n",
    "                grad_activation = self.Activations[-1].grad_feed(L_OUTPUTS[-1])\n",
    "                out_err = self.grad_crossentropy(L_OUTPUTS[-1],Y)*grad_activation\n",
    "            \n",
    "            \"\"\"\n",
    "                Backward Feeding\n",
    "            \"\"\"\n",
    "#             print('*Backward')\n",
    "            try :\n",
    "                for i in range(1,len(self.Layers)+1) :\n",
    "                    nam = self.Layers[-i].__Name__.lower()\n",
    "                    print(nam,out_err.shape)\n",
    "                    na = self.Activations[-i]\n",
    "                    if na is not None :\n",
    "                        if na.__Name__ != 'Softmax' :\n",
    "                            out_err = self.Activations[-i].feed_back(L_INPUTS[-i],out_err,lr)\n",
    "                    #print('->',out_err.shape,L_INPUTS[-i].shape)\n",
    "                    in_ = L_INPUTS[-i]\n",
    "                    if nam == 'dense_1' :\n",
    "                        in_ = in_.T\n",
    "                    w,b_,out_err = self.Layers[-i].feed_back(in_,out_err,lr,decay,opt=self.optimizer)\n",
    "                    self.Layers[-i].Batch_W.append(w)\n",
    "                    self.Layers[-i].Batch_B.append(b_)\n",
    "                    #print(w.shape,b_.shape,out_err.shape)\n",
    "                    \n",
    "            except Exception as e :\n",
    "                    print(e)\n",
    "                    pass\n",
    "#             print('*Done')\n",
    "            error = np.mean(loss)\n",
    "            print('\\r Loss : %f ' % (error) , end=\"\")\n",
    "            \n",
    "        for i in range(1,len(self.Layers)-1) :\n",
    "                Batch_W = np.array(self.Layers[-i].Batch_W)\n",
    "                Batch_B = np.array(self.Layers[-i].Batch_B)\n",
    "                self.Layers[-i].weights = np.mean(self.Layers[-i].Batch_W , axis = 0)\n",
    "                self.Layers[-i].bias = np.mean(self.Layers[-i].Batch_B)\n",
    "                self.Layers[-i].Batch_W = []\n",
    "                self.Layers[-i].Batch_B = []\n",
    "                \n",
    "        return error\n",
    "    \n",
    "    def fit(self,train_data,valid_data=None,validation_split=.1,epochs=10,lr=[1e-02],decay=1e-03,batch_size=8,model_updation_epoch=2,PlotView=10,Labels=None) :\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        input_data , output_data = None , None\n",
    "        val_input_data , val_target_data = None , None\n",
    "        if train_data is None :\n",
    "            raise ValueError('Training Data Required')\n",
    "        else :\n",
    "            input_data = train_data[0]\n",
    "            output_data = train_data[1]\n",
    "        N = len(input_data)\n",
    "        \n",
    "        if valid_data is None :\n",
    "            input_data , output_data , val_input_data , val_output_data = self.SplitData(input_data , output_data , validation_split)\n",
    "        else :\n",
    "            val_input_data , val_output_data = valid_data[0] , valid_data[1]\n",
    "        if Labels is not None :\n",
    "            self.Labels = Labels\n",
    "        print('\\nModel Fitting\\n')\n",
    "        \n",
    "        N = len(output_data)\n",
    "        \n",
    "        for ep in range(epochs) :\n",
    "            model.ep = ep + 1\n",
    "            start_ep = time.time()\n",
    "            error = 0\n",
    "            acc = 0\n",
    "            print(f'\\nepoch : {ep+1}/{epochs}')\n",
    "            \n",
    "            for b,batch in enumerate(tnrange(0,N-batch_size+1,batch_size,desc='batch')) :\n",
    "                loss = None\n",
    "                X , Y = input_data[batch:batch+batch_size] , output_data[batch : batch+batch_size]\n",
    "                \"\"\"\n",
    "                    Feed Forward\n",
    "                \"\"\"\n",
    "                outputs = X\n",
    "                outs = {i.__Name__ : {'lay':None,'act':None} for i in self.Layers}\n",
    "                ins = {i.__Name__ : {'lay':None,'act':None} for i in self.Layers}\n",
    "                \n",
    "                for lay , act in zip(self.Layers , self.Activations ) :\n",
    "                    ins[lay.__Name__]['lay'] = outputs\n",
    "                    outputs = lay.feed(outputs)\n",
    "                    outs[lay.__Name__]['lay'] = outputs\n",
    "                    if act is not None :\n",
    "                        ins[lay.__Name__]['act'] = outputs\n",
    "                        outputs = act.feed(outputs)\n",
    "                        outs[lay.__Name__]['act'] = outputs\n",
    "                    else :\n",
    "                        ins[lay.__Name__]['act'] = outputs\n",
    "                        outs[lay.__Name__]['act'] = outputs\n",
    "                        \n",
    "                \"\"\"\n",
    "                    Loss\n",
    "                \"\"\"\n",
    "                if self.loss == 'cross_entropy' :\n",
    "                    for i,j,k in zip(outs[self.Layers[-1].__Name__]['act'] , Y , outs[self.Layers[-1].__Name__]['lay']) :\n",
    "                        loss = self.cat_crossentropy(i,j,k)\n",
    "                        print('\\r Loss : %f ' % (loss) , end=\"\")\n",
    "                        if math.isnan(loss) :\n",
    "                            return None\n",
    "                        grad_activation = self.Activations[-1].grad_feed(k)\n",
    "                        out_err = self.grad_crossentropy(k,j)*grad_activation\n",
    "                \n",
    "                \"\"\"\n",
    "                    Backward Feed\n",
    "                \"\"\"\n",
    "                try :\n",
    "                    for b in range(batch_size) :\n",
    "                        for i in range(1,len(self.Layers)) :\n",
    "                            nam = self.Layers[-i].__Name__\n",
    "                            nam_ = self.Layers[-i-1].__Name__\n",
    "                            na = self.Activations[-i]\n",
    "                            na_ = self.Activations[-i-1]\n",
    "                            if na is not None :\n",
    "                                if na.__Name__ != 'Softmax' :\n",
    "                                    out_err = self.Activations[-i].feed_back(ins[nam]['act'][b],out_err,lr)\n",
    "                            if nam == 'Dense_1' :\n",
    "                                w,b_,out_err = self.Layers[-i].feed_back(np.expand_dims(outs[nam_]['act'][b],axis=0),out_err,lr,decay,opt=self.optimizer)\n",
    "                            else :\n",
    "                                w,b_,out_err = self.Layers[-i].feed_back(outs[nam_]['act'][b],out_err,lr,decay,opt=self.optimizer)\n",
    "                            self.Layers[-i].Batch_W.append(w)\n",
    "                            self.Layers[-i].Batch_B.append(b_)\n",
    "                    w,b_,out_err = self.Layers[-i].feed_back(ins['Conv2D_1']['act'][b],out_err,lr,decay,opt=self.optimizer)\n",
    "                    self.Layers[0].Batch_W.append(w)\n",
    "                    self.Layers[0].Batch_B.append(b_)\n",
    "                except Exception as e :\n",
    "                    print(e)\n",
    "                    break\n",
    "            accuracy = sum([y == np.argmax(model.predict(x)[0]) for x, y in zip(input_data, output_data)]) / N\n",
    "            self.acc.append(accuracy)\n",
    "            ac , loss = [] , []\n",
    "            for x, y in zip(val_input_data, val_output_data) :\n",
    "                out,lay_out = model.predict(x)\n",
    "                if y == np.argmax(out) :\n",
    "                    ac.append(1)\n",
    "                else :\n",
    "                    ac.append(0)\n",
    "                loss.append(self.cat_crossentropy(out,y,lay_out))\n",
    "            ac = np.mean(ac)\n",
    "            loss = np.mean(loss)\n",
    "            self.val_acc.append(ac)\n",
    "            print('\\racc=%f , loss=%f , val_acc=%f , val_loss=%f' % (accuracy , error , ac , loss))\n",
    "            \n",
    "            if ep > 3 :\n",
    "                if np.min(model.error) > error or np.min(model.val_error) > loss or np.max(model.acc[:-1]) < accuracy or np.max(model.val_acc[:-1]) < ac :\n",
    "                    callback(model,fileName)\n",
    "            else :\n",
    "                callback(model,fileName)\n",
    "                \n",
    "            self.error.append(error)\n",
    "            self.val_error.append(loss)\n",
    "            if not (ep+1)%PlotView :\n",
    "                PlotAcc(self)\n",
    "                PlotError(self)\n",
    "            \n",
    "            #end_ep = time.time()\n",
    "            \n",
    "            #print(f'Time Taken for epoch {ep+1} : {end_ep-start_ep}s')\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def mse(self,y_true, y_pred):\n",
    "        return np.mean(np.power(y_true - y_pred, 2))\n",
    "    \n",
    "    def mse_prime(self,y_true, y_pred):\n",
    "        return 2 * (y_pred - y_true) / y_pred.size\n",
    "    \n",
    "    def transfer_derivative(self,output):\n",
    "        return output * (1.0 - output)\n",
    "    \n",
    "    def binary_crossentropy(self,pred,Truth) :\n",
    "        GroundTruth = np.zeros(self.No_of_outs)\n",
    "        GroundTruth[Truth] = 1\n",
    "        return -np.mean(GroundTruth*np.log(pred)+(1-GroundTruth)*np.log(1-pred))\n",
    "    \n",
    "    def binary_grad_crossentropy(self,pred,Truth) :\n",
    "        GroundTruth = np.zeros(self.No_of_outs)\n",
    "        GroundTruth[Truth] = 1\n",
    "        return -((GroundTruth/pred)-((1-GroundTruth)/(1-pred)))\n",
    "    \n",
    "    def cat_crossentropy(self,pred,Truth,out) :\n",
    "        Truth = self.one_hot_encode(Truth)\n",
    "        return np.dot(Truth.ravel()/pred.ravel() , self.Activations[-1].grad_feed(out).ravel())\n",
    "\n",
    "    def grad_cat_crossentropy(self,pred,Truth) :\n",
    "        Truth = self.one_hot_encode(Truth)\n",
    "        a = Truth/pred\n",
    "        b = self.Activations[-1].grad_feed(self.Layers[-1].output)[0]\n",
    "        return np.dot((Truth/pred),self.Activations[-1].grad_feed(self.Layers[-1].output)[0])\n",
    "    \n",
    "    def crossentropy(self,logits,reference_answers) :\n",
    "        return - logits[0][reference_answers] + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    def grad_crossentropy(self,logits,reference_answers):\n",
    "        ones_for_answers = np.zeros_like(logits)\n",
    "        ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "        e_x = np.exp(logits-np.max(logits))\n",
    "        softmax = e_x / e_x.sum(axis=-1,keepdims=True)\n",
    "        return (- ones_for_answers + softmax) / logits.shape[0]\n",
    "    \n",
    "    def showImg(self,X) :\n",
    "        plt.imshow(X)\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self,X):\n",
    "        outputs = []\n",
    "        lay_out = None\n",
    "        #print(X.shape , model.input_shape)\n",
    "        if X.shape == model.input_shape[1:] :\n",
    "            output = X\n",
    "            for layer , activation in zip(self.Layers,self.Activations) :\n",
    "                output = layer.predict(output)\n",
    "                lay_out = output\n",
    "                if activation is not None :\n",
    "                    output = activation.predict(output)\n",
    "            outputs.append(output)\n",
    "        else :\n",
    "            for output in X :\n",
    "                for layer , activation in zip(self.Layers,self.Activations) :\n",
    "                    output = layer.predict(output)\n",
    "                    if activation is not None :\n",
    "                        output = activation.feed(output)\n",
    "                outputs.append(output)\n",
    "        return np.array(outputs) , lay_out\n",
    "    \n",
    "    def pred_class(self,X) :\n",
    "        classes = []\n",
    "        if X.shape == model.input_shape :\n",
    "            output = self.predict(X)\n",
    "            return np.argmax(output)\n",
    "        else :\n",
    "            for output in X :\n",
    "                output = self.predict(output)\n",
    "                classes.append(np.argmax(output))\n",
    "            return np.array(classes)\n",
    "    \n",
    "    def Summary(self) :\n",
    "        print('='*60)\n",
    "        print('Model Summary')\n",
    "        print('_'*60)\n",
    "        print('Layers',' '*(20-len('Layers')),'Input Shape',' '*(20-len('Input Shape')),'Output Shape',' '*(20-len('Output Shape')))\n",
    "        print('='*60)\n",
    "        for Layer in self.Layers :\n",
    "            if Layer.__type__ != 'activation' :\n",
    "                Layer.Summary()\n",
    "                print('_'*60)\n",
    "        print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(PATH,size=32,channels=3) :\n",
    "    Labels = []\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for i in os.listdir(PATH) :\n",
    "        print(f'\\r\\t {i} \\t' ,end=\"\")\n",
    "        Labels.append(i)\n",
    "        co = 0\n",
    "        for j in os.listdir(PATH + '/' + i) :\n",
    "            try :\n",
    "                img = cv2.imread(PATH + '/' + i + '/' + j)\n",
    "                img = cv2.resize(img , (size,size))\n",
    "                #img = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY)\n",
    "                X_train.append(img)\n",
    "                Y_train.append(Labels.index(i))\n",
    "                co += 1\n",
    "            except :\n",
    "                pass\n",
    "            if co == 30 :\n",
    "                break\n",
    "        print(f'\\r{i} - {len(set(Y_train))} - {len(Y_train)}')\n",
    "\n",
    "    X_train , Y_train = np.array(X_train)/255. , np.array(Y_train)\n",
    "    return X_train , Y_train , Labels\n",
    "\n",
    "# PATH = 'D:/Data/Traffic Signs/Custom/Signals/'\n",
    "# X_train , Y_train , Labels = LoadData(PATH)\n",
    "# loc = 'LoadedData_New.obj'\n",
    "# Data = (X_train,Y_train,Labels)\n",
    "# f = open(loc , 'wb')\n",
    "# pickle.dump(Data , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 'LoadedData_New.obj'\n",
    "f = open(loc , 'rb')\n",
    "X_train , Y_train , Labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16,2,input_shape=X_train[0].shape,A_F='ReLU'))\n",
    "model.add(MaxPool2D(3,input_shape=model.output_shape))\n",
    "model.add(Flatten(input_shape=model.output_shape))\n",
    "model.add(Dense(input_shape=model.output_shape,N_F=100,A_F='ReLU')) ### Input Layer\n",
    "model.add(Dropout(input_shape=model.output_shape))\n",
    "model.add(Dense(input_shape=model.output_shape,N_F=len(set(Y_train)),A_F='Softmax')) ### Output Layer\n",
    "\n",
    "model.compile(optimizer='adam',loss='cross_entropy',metrics=['acc'])\n",
    "\n",
    "model.Summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Fitting\n",
      "\n",
      "\n",
      "epoch : 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac5bd32d46443b19dd23fd8d0ec6115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch', max=324.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss : 0.992679 \n",
      "acc=0.086420 , loss=0.000000 , val_acc=0.055556 , val_loss=0.944251\n",
      "CallBack :  Model_Conv_2.0.h5 Updated\n",
      "\n",
      "epoch : 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eebbc7e5a3f4cbcbb9a4843e6a8026f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch', max=324.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss : 0.992679 \n",
      "acc=0.086420 , loss=0.000000 , val_acc=0.055556 , val_loss=0.944251\n",
      "CallBack :  Model_Conv_2.0.h5 Updated\n",
      "\n",
      "epoch : 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7dc64cfa20841578b8858a48c52fe9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch', max=324.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss : 0.992679 \n",
      "acc=0.086420 , loss=0.000000 , val_acc=0.055556 , val_loss=0.944251\n",
      "CallBack :  Model_Conv_2.0.h5 Updated\n",
      "\n",
      "epoch : 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151857e7cc774757b302ce66186791c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch', max=324.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss : 0.992679 \n",
      "acc=0.086420 , loss=0.000000 , val_acc=0.055556 , val_loss=0.944251\n",
      "CallBack :  Model_Conv_2.0.h5 Updated\n",
      "\n",
      "epoch : 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c8f9fb3d7e4f45908da36fe1f1b896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch', max=324.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss : 0.992679 \n",
      "acc=0.086420 , loss=0.000000 , val_acc=0.055556 , val_loss=0.944251\n",
      "\n",
      "epoch : 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73b9a2feac24857a932051406b374ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch', max=324.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss : 0.992679 \n",
      "acc=0.086420 , loss=0.000000 , val_acc=0.055556 , val_loss=0.944251\n",
      "\n",
      "epoch : 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ba06cb4f264ff9805d47fb47d28716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch', max=324.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss : 0.992679 \n",
      "acc=0.086420 , loss=0.000000 , val_acc=0.055556 , val_loss=0.944251\n",
      "\n",
      "epoch : 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402c03a51c394b74bcae71c2416593ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch', max=324.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss : 0.992679 \n",
      "acc=0.086420 , loss=0.000000 , val_acc=0.055556 , val_loss=0.944251\n",
      "\n",
      "epoch : 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ca5c676a0845309979d439c7c63bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch', max=324.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss : 0.992679 \n",
      "acc=0.086420 , loss=0.000000 , val_acc=0.055556 , val_loss=0.944251\n",
      "\n",
      "epoch : 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4de9f35f636449fb41b00fd852dff41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batch', max=324.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss : 0.992679 \n",
      "acc=0.086420 , loss=0.000000 , val_acc=0.055556 , val_loss=0.944251\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARTklEQVR4nO3df4xV9ZmA8ecVRrFFtlZRhFHBBEUKFbsD1W3E1e6KtlZi21T8gZEYDbWimC2LbqMxtU13211LN6US4iI10hWiZJetLOwfulLT1jDQUUSUECo44NaBVdfUEATe/WOm7TAMM3fwDpf5zvNJSLj3fOfed07g4XDm3nsiM5Ek9X3H1XoASVJ1GHRJKoRBl6RCGHRJKoRBl6RCDKzVE5966qk5cuTIWj29JPVJ69at25WZQzvbVrOgjxw5ksbGxlo9vST1SRGx7XDbPOUiSYUw6JJUCIMuSYUw6JJUCIMuSYUw6JJUCIMuSYWo2evQj9js2dDUVOspJOnITZgA8+ZV/WE9QpekQvS9I/Re+FdNkkrgEbokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFaKioEfElRHxekRsiYh7O9n+ZxHxHxHxUkRsjIgZ1R9VktSVboMeEQOA+cBVwFjg+ogY22HZN4BXM/MC4C+Bf4qI46s8qySpC5UcoU8CtmTm1szcCzwJTO2wJoGTIiKAwcD/AvuqOqkkqUuVBH0E8Ga7281t97X3Y+B8YCewAbg7Mw90fKCIuD0iGiOisaWl5QhHliR1ppKgRyf3ZYfbU4AmYDgwAfhxRAw55IsyF2ZmQ2Y2DB06tMfDSpIOr5KgNwNntrtdT+uReHszgOXZagvwW2BMdUaUJFWikqCvBUZHxKi2H3ROA1Z0WLMd+DxARJwOnAdsreagkqSuDexuQWbui4g7gdXAAGBRZm6MiJlt2xcADwGLI2IDrado5mbmrl6cW5LUQbdBB8jMlcDKDvctaPf7ncAV1R1NktQTvlNUkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEAZdkgpRUdAj4sqIeD0itkTEvYdZ85cR0RQRGyPi+eqOKUnqzsDuFkTEAGA+8NdAM7A2IlZk5qvt1nwC+AlwZWZuj4jTemtgSVLnKjlCnwRsycytmbkXeBKY2mHNDcDyzNwOkJlvV3dMSVJ3Kgn6CODNdreb2+5r71zg5Ij474hYFxE3d/ZAEXF7RDRGRGNLS8uRTSxJ6lQlQY9O7ssOtwcCfw58EZgC3B8R5x7yRZkLM7MhMxuGDh3a42ElSYfX7Tl0Wo/Iz2x3ux7Y2cmaXZn5e+D3EbEGuADYXJUpJUndquQIfS0wOiJGRcTxwDRgRYc1/w5cEhEDI+JjwGeBTdUdVZLUlW6P0DNzX0TcCawGBgCLMnNjRMxs274gMzdFxCrgZeAA8GhmvtKbg0uSDhaZHU+HHx0NDQ3Z2NhYk+eWpL4qItZlZkNn23ynqCQVwqBLUiEMuiQVwqBLUiEqeR26JFXNhx9+SHNzM3v27Kn1KMe0QYMGUV9fT11dXcVfY9AlHVXNzc2cdNJJjBw5kojO3oiuzGT37t00NzczatSoir/OUy6Sjqo9e/ZwyimnGPMuRASnnHJKj/8XY9AlHXXGvHtHso8MuiQVwqBL6ncGDx5c6xF6hUGXpEIYdEn9VmYyZ84cxo0bx/jx41m6dCkAb731FpMnT2bChAmMGzeOX/ziF+zfv59bbrnlj2t/+MMf1nj6Q/myRUk1M3s2NDVV9zEnTIB58ypbu3z5cpqamnjppZfYtWsXEydOZPLkyfzsZz9jypQpfOtb32L//v188MEHNDU1sWPHDl55pfWDZN99993qDl4FHqFL6rdeeOEFrr/+egYMGMDpp5/OpZdeytq1a5k4cSKPPfYYDz74IBs2bOCkk07inHPOYevWrcyaNYtVq1YxZMiQWo9/CI/QJdVMpUfSveVwHx8+efJk1qxZwzPPPMP06dOZM2cON998My+99BKrV69m/vz5LFu2jEWLFh3libvmEbqkfmvy5MksXbqU/fv309LSwpo1a5g0aRLbtm3jtNNO47bbbuPWW29l/fr17Nq1iwMHDvCVr3yFhx56iPXr19d6/EN4hC6p37r22mv51a9+xQUXXEBE8P3vf59hw4bx05/+lB/84AfU1dUxePBgHn/8cXbs2MGMGTM4cOAAAN/73vdqPP2hvGKRpKNq06ZNnH/++bUeo0/obF95xSJJ6gcMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiR1oavPTn/jjTcYN27cUZymawZdkgrhW/8l1U4NPj937ty5nH322dxxxx0APPjgg0QEa9as4Z133uHDDz/kO9/5DlOnTu3R0+7Zs4evf/3rNDY2MnDgQB5++GEuu+wyNm7cyIwZM9i7dy8HDhzg6aefZvjw4Xzta1+jubmZ/fv3c//993Pdddd9pG8bDLqkfmbatGnMnj37j0FftmwZq1at4p577mHIkCHs2rWLiy66iGuuuaZHF2qeP38+ABs2bOC1117jiiuuYPPmzSxYsIC7776bG2+8kb1797J//35WrlzJ8OHDeeaZZwB47733qvK9GXRJtVODz8+98MILefvtt9m5cyctLS2cfPLJnHHGGdxzzz2sWbOG4447jh07dvC73/2OYcOGVfy4L7zwArNmzQJgzJgxnH322WzevJmLL76Y7373uzQ3N/PlL3+Z0aNHM378eL75zW8yd+5crr76ai655JKqfG+eQ5fU73z1q1/lqaeeYunSpUybNo0lS5bQ0tLCunXraGpq4vTTT2fPnj09eszDfdDhDTfcwIoVKzjxxBOZMmUKzz77LOeeey7r1q1j/Pjx3HfffXz729+uxrflEbqk/mfatGncdttt7Nq1i+eff55ly5Zx2mmnUVdXx3PPPce2bdt6/JiTJ09myZIlXH755WzevJnt27dz3nnnsXXrVs455xzuuusutm7dyssvv8yYMWP45Cc/yU033cTgwYNZvHhxVb4vgy6p3/nUpz7F+++/z4gRIzjjjDO48cYb+dKXvkRDQwMTJkxgzJgxPX7MO+64g5kzZzJ+/HgGDhzI4sWLOeGEE1i6dClPPPEEdXV1DBs2jAceeIC1a9cyZ84cjjvuOOrq6njkkUeq8n1V9HnoEXEl8CNgAPBoZv79YdZNBH4NXJeZT3X1mH4eutQ/+Xnolav656FHxABgPnAVMBa4PiLGHmbdPwCrj2BuSdJHVMkpl0nAlszcChARTwJTgVc7rJsFPA1MrOqEklRjGzZsYPr06Qfdd8IJJ/Diiy/WaKLOVRL0EcCb7W43A59tvyAiRgDXApfTRdAj4nbgdoCzzjqrp7NKKkRm9ug13rU2fvx4mqr9BqhuHMnlQSt52WJne73jM80D5mbm/q4eKDMXZmZDZjYMHTq00hklFWTQoEHs3r37iILVX2Qmu3fvZtCgQT36ukqO0JuBM9vdrgd2dljTADzZ9i/uqcAXImJfZv5bj6aRVLz6+nqam5tpaWmp9SjHtEGDBlFfX9+jr6kk6GuB0RExCtgBTANuaL8gM0f94fcRsRj4uTGX1Jm6ujpGjRrV/UL1WLdBz8x9EXEnra9eGQAsysyNETGzbfuCXp5RklSBit5YlJkrgZUd7us05Jl5y0cfS5LUU36WiyQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVwqBLUiEMuiQVoqKgR8SVEfF6RGyJiHs72X5jRLzc9uuXEXFB9UeVJHWl26BHxABgPnAVMBa4PiLGdlj2W+DSzPw08BCwsNqDSpK6VskR+iRgS2Zuzcy9wJPA1PYLMvOXmflO281fA/XVHVOS1J1Kgj4CeLPd7ea2+w7nVuA/O9sQEbdHRGNENLa0tFQ+pSSpW5UEPTq5LztdGHEZrUGf29n2zFyYmQ2Z2TB06NDKp5QkdWtgBWuagTPb3a4HdnZcFBGfBh4FrsrM3dUZT5JUqUqO0NcCoyNiVEQcD0wDVrRfEBFnAcuB6Zm5ufpjSpK60+0Rembui4g7gdXAAGBRZm6MiJlt2xcADwCnAD+JCIB9mdnQe2NLkjqKzE5Ph/e6hoaGbGxsrMlzS1JfFRHrDnfA7DtFJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQBl2SCmHQJakQFQU9Iq6MiNcjYktE3NvJ9oiIf27b/nJEfKb6o0qSutJt0CNiADAfuAoYC1wfEWM7LLsKGN3263bgkSrPKUnqxsAK1kwCtmTmVoCIeBKYCrzabs1U4PHMTODXEfGJiDgjM9+q9sCzZ0NTU7UfVZKOngkTYN686j9uJadcRgBvtrvd3HZfT9cQEbdHRGNENLa0tPR0VklSFyo5Qo9O7ssjWENmLgQWAjQ0NByyvRK98a+aJJWgkiP0ZuDMdrfrgZ1HsEaS1IsqCfpaYHREjIqI44FpwIoOa1YAN7e92uUi4L3eOH8uSTq8bk+5ZOa+iLgTWA0MABZl5saImNm2fQGwEvgCsAX4AJjReyNLkjpTyTl0MnMlrdFuf9+Cdr9P4BvVHU2S1BO+U1SSCmHQJakQBl2SCmHQJakQ0frzzBo8cUQLsO0Iv/xUYFcVx+nr3B8Hc3/8ifviYCXsj7Mzc2hnG2oW9I8iIhozs6HWcxwr3B8Hc3/8ifviYKXvD0+5SFIhDLokFaKvBn1hrQc4xrg/Dub++BP3xcGK3h998hy6JOlQffUIXZLUgUGXpEL0uaB3d8Hq/iQizoyI5yJiU0RsjIi7az1TrUXEgIj4TUT8vNaz1FrbpSCfiojX2v6MXFzrmWolIu5p+zvySkT8a0QMqvVMvaFPBb3CC1b3J/uAv8nM84GLgG/08/0BcDewqdZDHCN+BKzKzDHABfTT/RIRI4C7gIbMHEfrx4BPq+1UvaNPBZ12F6zOzL3AHy5Y3S9l5luZub7t9+/T+hf2kGu59hcRUQ98EXi01rPUWkQMASYD/wKQmXsz893aTlVTA4ETI2Ig8DEKvaJaXwt6RRej7o8iYiRwIfBibSepqXnA3wIHaj3IMeAcoAV4rO0U1KMR8fFaD1ULmbkD+EdgO/AWrVdU+6/aTtU7+lrQK7oYdX8TEYOBp4HZmfl/tZ6nFiLiauDtzFxX61mOEQOBzwCPZOaFwO+Bfvkzp4g4mdb/yY8ChgMfj4ibajtV7+hrQfdi1B1ERB2tMV+SmctrPU8NfQ64JiLeoPVU3OUR8URtR6qpZqA5M//wP7anaA18f/RXwG8zsyUzPwSWA39R45l6RV8LeiUXrO43IiJoPUe6KTMfrvU8tZSZ92VmfWaOpPXPxbOZWeRRWCUy83+ANyPivLa7Pg+8WsORamk7cFFEfKzt78znKfQHxBVdU/RYcbgLVtd4rFr6HDAd2BARTW33/V3bNWClWcCStoOfrfTTi7dn5osR8RSwntZXhv2GQj8CwLf+S1Ih+topF0nSYRh0SSqEQZekQhh0SSqEQZekQhh0SSqEQZekQvw/ZpmrSndVEOAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model.fit(train_data=(X_train,Y_train),\n",
    "          epochs=10,\n",
    "          #valid_data=(X_test,Y_test),\n",
    "          lr=[0.01],\n",
    "          decay=1e-08,\n",
    "          batch_size=2,\n",
    "          PlotView=10,\n",
    "          Labels=Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveModel(obj,fileName) :\n",
    "    f = open(fileName, 'wb') \n",
    "    pickle.dump(obj, f)\n",
    "    \n",
    "def loadModel(fileName) :\n",
    "    f = open(fileName, 'rb') \n",
    "    model = pickle.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PlotAcc(model)\n",
    "PlotError(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'Model_Conv_1.0.h5'\n",
    "model = loadModel(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1000 is out of bounds for axis 0 with size 360",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b001a724172d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mLabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1000 is out of bounds for axis 0 with size 360"
     ]
    }
   ],
   "source": [
    "c = 1000\n",
    "plt.imshow(X_train[c])\n",
    "plt.show()\n",
    "Labels[model.pred_class(X_train[c])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in range(len(X_train)) :\n",
    "    #print(model.pred_class(X_train[i]),Y_train[i])\n",
    "    if model.pred_class(X_train[i]) == Y_train[i] :\n",
    "        c += 1\n",
    "print(c*100/len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
